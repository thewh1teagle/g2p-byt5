# Simple Pre-tokenization for G2P

## Core Insight

**The model should see the FULL sentence** and learn to:
1. Convert Hebrew characters â†’ phonemes
2. Preserve everything else exactly as-is (punctuation, spaces, emojis, English, numbers)

This is **simpler** than the junior's approach because:
- No `<UNK>` tokens needed
- No position tracking needed
- No reconstruction complexity
- Model does all the work

## How It Works

### Training

The model is trained on pairs like:

```
Input:  "×©×œ×•× ×¢×•×œ×!"
Output: "Êƒalom olam!"
```

```
Input:  "×××¨: '×©×œ×•×'"
Output: "amar: 'Êƒalom'"
```

```
Input:  "Hello ×©×œ×•× ğŸ˜Š"
Output: "Hello Êƒalom ğŸ˜Š"
```

The model learns a **character-level transformation**:
- Hebrew letters â†’ phoneme characters
- Everything else â†’ identity (stays the same)

### Inference

```python
# User input
text = "×©×œ×•×ğŸ˜Š×¢×•×œ×!"

# Normalize quotes only (optional)
normalized = normalize_quotes(text)  # "×©×œ×•×ğŸ˜Š×¢×•×œ×!"

# Model sees full sentence
model_output = model.generate(normalized)  # "ÊƒalomğŸ˜Šolam!"

# That's it! No reconstruction needed.
```

## Why This is Better

### Junior's Approach (Complex)
```
Input: "×©×œ×•×ğŸ˜Š×¢×•×œ×!"
â†“
Preprocess: "×©×œ×•× <UNK> ×¢×•×œ× !" + tracking {0: "ğŸ˜Š"}
â†“
Model: "Êƒalom <UNK> olam !"
â†“
Restore UNK: "ÊƒalomğŸ˜Šolam !"
â†“
Remove spacing: "ÊƒalomğŸ˜Šolam!"
```

**Problems:**
- Need to track UNK positions
- Need to restore UNK content
- Need to remove punctuation spacing
- Model doesn't see emojis/English in training
- Complex edge cases

### Our Approach (Simple)
```
Input: "×©×œ×•×ğŸ˜Š×¢×•×œ×!"
â†“
Model: "ÊƒalomğŸ˜Šolam!"
```

**Benefits:**
- âœ… Model sees actual content (learns to preserve it)
- âœ… No tracking needed
- âœ… No reconstruction needed
- âœ… Handles any character naturally
- âœ… Model has full context

## What the Model Learns

The model learns a **context-aware character mapping**:

| Input Char | Output | Context |
|------------|--------|---------|
| `×©` | `Êƒ` | Hebrew letter |
| `×œ` | `l` | Hebrew letter |
| `×•` | `o` | Hebrew letter (context-dependent) |
| `×` | `m` | Hebrew letter |
| `ğŸ˜Š` | `ğŸ˜Š` | Non-Hebrew (identity) |
| `!` | `!` | Punctuation (identity) |
| ` ` | ` ` | Space (identity) |
| `H` | `H` | Latin letter (identity) |
| `1` | `1` | Digit (identity) |

**Key:** Hebrew letters are in the range `\u05d0-\u05ea` (×-×ª). Everything else is identity.

## Training Data Format

### From Word Pairs to Sentences

If you have word-level data:
```
×©×œ×•× â†’ Êƒalom
×¢×•×œ× â†’ olam
```

Create sentence-level training data:

**Option 1: Keep it simple (individual words)**
```
Input:  "×©×œ×•×"
Output: "Êƒalom"
```

**Option 2: Add context (with punctuation)**
```
Input:  "×©×œ×•×."
Output: "Êƒalom."

Input:  "×©×œ×•×!"
Output: "Êƒalom!"
```

**Option 3: Combine into sentences**
```
Input:  "×©×œ×•× ×¢×•×œ×"
Output: "Êƒalom olam"

Input:  "×©×œ×•× ×¢×•×œ×!"
Output: "Êƒalom olam!"
```

**Option 4: Add non-Hebrew content**
```
Input:  "Hello ×©×œ×•×"
Output: "Hello Êƒalom"

Input:  "×©×œ×•× ğŸ˜Š"
Output: "Êƒalom ğŸ˜Š"
```

The model will learn to:
- Transform Hebrew â†’ phonemes
- Preserve everything else

## Implementation

### Core Functions

```python
def normalize_quotes(text: str) -> str:
    """Optional: Normalize Hebrew/curly quotes to ASCII."""
    # '×³' â†’ "'"
    # '×´' â†’ '"'
    # etc.

def g2p_pipeline(text: str) -> str:
    """Full pipeline: normalize â†’ model â†’ done."""
    normalized = normalize_quotes(text)
    return model.generate(normalized)
```

That's it! No complex preprocessing or reconstruction.

### Model Training

```python
# Prepare training data
train_pairs = []
for hebrew_word, phonemes in word_pairs:
    # Option 1: Just words
    train_pairs.append((hebrew_word, phonemes))

    # Option 2: Add variations
    train_pairs.append((f"{hebrew_word}.", f"{phonemes}."))
    train_pairs.append((f"{hebrew_word}!", f"{phonemes}!"))

    # Option 3: Combine into sentences
    # (pair with other words randomly)

# Train ByT5 model
model.train(train_pairs)
```

### Model Inference

```python
def phonemize(text: str) -> str:
    """Convert Hebrew text to phonemes."""
    text = normalize_quotes(text)
    return model.generate(text)
```

## Edge Cases

All handled naturally by the model:

| Input | Output | Notes |
|-------|--------|-------|
| `"×©×œ×•×ğŸ˜Š"` | `"ÊƒalomğŸ˜Š"` | Emoji preserved |
| `"×©×œ×•× 123"` | `"Êƒalom 123"` | Numbers preserved |
| `"Hello ×©×œ×•×"` | `"Hello Êƒalom"` | English preserved |
| `"×©×œ×•×!"` | `"Êƒalom!"` | Punctuation preserved |
| `"  ×©×œ×•×  "` | `"  Êƒalom  "` | Spaces preserved |
| `"×Ö¾×‘"` | `"a-b"` | Hyphens preserved |

## Comparison

### Complexity

| Aspect | Junior's Approach | Our Approach |
|--------|------------------|--------------|
| Preprocessing | Complex (UNK tracking) | Simple (quote normalization) |
| Model Input | Modified (`<UNK>`) | Original (full content) |
| Model Training | Learns to preserve `<UNK>` | Learns to preserve actual content |
| Reconstruction | Complex (restore, remove spacing) | None needed |
| Edge Cases | Many (UNK positioning, spacing) | None (model handles all) |

### Code Size

| Component | Junior's | Ours |
|-----------|----------|------|
| `preprocess` | ~100 lines | ~15 lines |
| `reconstruct` | ~50 lines | ~0 lines |
| `pipeline` | ~40 lines | ~10 lines |
| **Total** | **~190 lines** | **~25 lines** |

### Training Data

| Approach | Data Format | Model Learns |
|----------|-------------|--------------|
| Junior's | `"×©×œ×•× <UNK> ×¢×•×œ×" â†’ "Êƒalom <UNK> olam"` | Hebrew + `<UNK>` placeholder |
| Ours | `"×©×œ×•×ğŸ˜Š×¢×•×œ×" â†’ "ÊƒalomğŸ˜Šolam"` | Hebrew + actual content |

Our approach is **more robust** because the model sees real examples during training.

## Summary

1. **Model sees full sentences** â†’ Better context for phonemization
2. **No special tokens** â†’ Model learns real content preservation
3. **No reconstruction** â†’ Output is final result
4. **Simpler code** â†’ Fewer bugs, easier maintenance
5. **Better training** â†’ Model sees actual non-Hebrew content

The key insight: **Let the model do the work**. ByT5 is designed for character-level transformations. Just train it on the full content and it will learn to preserve what it shouldn't change.

## If Your Dataset is Clean (Hebrew + Punctuation Only)

**Good news:** You need to do **even less work**!

### Scenario: Your data only has Hebrew words and punctuation

If your existing dataset looks like this:
```
×©×œ×•× â†’ Êƒalom
×¢×•×œ× â†’ olam
×©×œ×•×! â†’ Êƒalom!
××”? â†’ ma?
```

**You can train directly with ZERO preprocessing!**

### Why this works

ByT5 will learn the character mapping:
- Hebrew letters â†’ phonemes
- Punctuation â†’ identity (stays the same)
- Spaces â†’ identity (stays the same)

### What about emojis/English that users might input later?

The model will **automatically generalize** because:

1. **During training:** Model learns "only transform Hebrew letters"
2. **At inference:** When it sees emoji/English, it doesn't match any Hebrew letter, so it outputs it as-is

**Example:**
```python
# Training data (clean, only Hebrew + punctuation)
train_pairs = [
    ("×©×œ×•×", "Êƒalom"),
    ("×¢×•×œ×", "olam"),
    ("×©×œ×•×!", "Êƒalom!"),
]

# Train model
model.train(train_pairs)

# At inference - model automatically handles unseen characters!
model.generate("×©×œ×•×")       # â†’ "Êƒalom"  âœ“ (trained)
model.generate("×©×œ×•×ğŸ˜Š")      # â†’ "ÊƒalomğŸ˜Š" âœ“ (emoji passes through)
model.generate("Hello ×©×œ×•×")  # â†’ "Hello Êƒalom" âœ“ (English passes through)
model.generate("×©×œ×•× 123")    # â†’ "Êƒalom 123" âœ“ (numbers pass through)
```

### Why the model generalizes

ByT5 is a **character-level** model. It learns patterns like:

| Input char | Output char | Pattern learned |
|------------|-------------|-----------------|
| `×©` | `Êƒ` | Hebrew letter â†’ transform |
| `×œ` | `l` | Hebrew letter â†’ transform |
| `!` | `!` | Punctuation â†’ identity |
| ` ` | ` ` | Space â†’ identity |
| `ğŸ˜Š` | `ğŸ˜Š` | Unknown â†’ identity (no training example, so copy) |
| `H` | `H` | Unknown â†’ identity (no training example, so copy) |

The model learns: "Transform Hebrew letters, copy everything else."

### Do you need to add non-Hebrew examples?

**No, but you can if you want to be extra safe.**

#### Option 1: Train on clean data only (recommended)
```python
# Just use your existing clean dataset
train_pairs = load_clean_hebrew_data()  # Only Hebrew + punctuation
model.train(train_pairs)
```

**Pros:**
- âœ… No extra work
- âœ… Model will generalize naturally
- âœ… Simpler training data

**Cons:**
- âš ï¸ Not 100% guaranteed to preserve unseen characters (but very likely)

#### Option 2: Add a few non-Hebrew examples (extra safety)
```python
# Start with your clean data
train_pairs = load_clean_hebrew_data()

# Add a handful of examples with non-Hebrew content
train_pairs.extend([
    ("Hello ×©×œ×•×", "Hello Êƒalom"),
    ("×©×œ×•×ğŸ˜Š", "ÊƒalomğŸ˜Š"),
    ("×©×œ×•× 123", "Êƒalom 123"),
    ("test ×©×œ×•× test", "test Êƒalom test"),
])

model.train(train_pairs)
```

**Pros:**
- âœ… Explicitly teaches model to preserve non-Hebrew
- âœ… More robust
- âœ… Only need ~10-20 examples for this

**Cons:**
- âš ï¸ Requires creating synthetic examples

### Recommendation

**If your dataset is clean (Hebrew + punctuation only):**

1. **First:** Train on your clean data as-is (no preprocessing needed!)
2. **Test:** Try some examples with emojis/English
3. **If needed:** Add 10-20 synthetic examples with non-Hebrew content

Most likely, step 1 will be sufficient. ByT5 is designed to generalize well.
